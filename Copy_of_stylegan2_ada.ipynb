{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlDbqnB1vYGi",
        "outputId": "8f668c31-c697-43e5-ea8b-7f66188c7f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla K80 (UUID: GPU-fe0ddd8e-ffc6-9c06-0677-51f87ad46062)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl8MBjcsee_p",
        "outputId": "3461012f-f0a9-4e51-9c97-b29bf72d57f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J811ggDNQQN9",
        "outputId": "12d9c69d-577a-43d6-b671-09937ed11cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (128/128), 1.12 MiB | 7.22 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "/content/stylegan2-ada-pytorch\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch\n",
        "%cd /content/stylegan2-ada-pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Rxq-hpCvvG5"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/AdobeStock-cropped-512x512 /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnQ8w8dVsIiY",
        "outputId": "7e519ce7-9cda-4271-b74f-408b76e9d395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "  3% 56/2088 [00:01<00:56, 35.71it/s]Error: Image 00000/img00000056.png attributes must be equal across all images of the dataset.  Got:\n",
            "  dataset width/cur image width: 512/512\n",
            "  dataset height/cur image height: 512/512\n",
            "  dataset channels/cur image channels: 3/1\n",
            "  3% 56/2088 [00:01<00:58, 34.58it/s]\n"
          ]
        }
      ],
      "source": [
        "# dataset preparation, one time step\n",
        "%cd /content/\n",
        "# !unzip curated-1-resized-256x256.zip\n",
        "!python /content/stylegan2-ada-pytorch/dataset_tool.py --source=/content/drive/MyDrive/AdobeStock-cropped-512x512 --dest=/content/train-data\n",
        "!cp -r /content/train-data /content/drive/MyDrive/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cqqZ_RyzrCD",
        "outputId": "a9579a87-f443-49b0-cc7f-72f02c2dece9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.7.1 in /usr/local/lib/python3.7/dist-packages (1.7.1)\n",
            "Requirement already satisfied: torchvision==0.8.2 in /usr/local/lib/python3.7/dist-packages (0.8.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.7/dist-packages (1.10.2.3)\n",
            "Requirement already satisfied: opensimplex in /usr/local/lib/python3.7/dist-packages (0.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.21.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.7.1 torchvision==0.8.2 ninja opensimplex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkdEwDLRjWkJ",
        "outputId": "556f2abe-4651-48c7-a39e-970db56f9b4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-03-13 20:19:51--  https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res256-mirror-paper256-noaug.pkl\n",
            "Resolving nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)... 108.159.227.82, 108.159.227.98, 108.159.227.118, ...\n",
            "Connecting to nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)|108.159.227.82|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 295744285 (282M) [binary/octet-stream]\n",
            "Saving to: ‘ffhq-res256-mirror-paper256-noaug.pkl’\n",
            "\n",
            "ffhq-res256-mirror- 100%[===================>] 282.04M  41.1MB/s    in 8.8s    \n",
            "\n",
            "2022-03-13 20:20:00 (31.9 MB/s) - ‘ffhq-res256-mirror-paper256-noaug.pkl’ saved [295744285/295744285]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res256-mirror-paper256-noaug.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZNoYKJesFe5",
        "outputId": "13a7ab12-43a3-4a2e-f2df-263b336a7336"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/stylegan2-ada-pytorch\n",
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 20,\n",
            "  \"network_snapshot_ticks\": 20,\n",
            "  \"metrics\": [\n",
            "    \"fid50k_full\"\n",
            "  ],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/ASdest8\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 897,\n",
            "    \"xflip\": true,\n",
            "    \"resolution\": 256\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 2\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 16384,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 0.8192\n",
            "  },\n",
            "  \"total_kimg\": 25000,\n",
            "  \"batch_size\": 32,\n",
            "  \"batch_gpu\": 32,\n",
            "  \"ema_kimg\": 5.0,\n",
            "  \"ema_rampup\": 0.05,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1,\n",
            "    \"brightness\": 1,\n",
            "    \"contrast\": 1,\n",
            "    \"lumaflip\": 1,\n",
            "    \"hue\": 1,\n",
            "    \"saturation\": 1\n",
            "  },\n",
            "  \"run_dir\": \"/content/drive/MyDrive/AS-out-dir/00000-ASdest8-mirror-auto1-batch32\"\n",
            "}\n",
            "\n",
            "Output directory:   /content/drive/MyDrive/AS-out-dir/00000-ASdest8-mirror-auto1-batch32\n",
            "Training data:      /content/ASdest8\n",
            "Training duration:  25000 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   897\n",
            "Image resolution:   256\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    True\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "\n",
            "Num images:  1794\n",
            "Image shape: [3, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator             Parameters  Buffers  Output shape         Datatype\n",
            "---                   ---         ---      ---                  ---     \n",
            "mapping.fc0           262656      -        [32, 512]            float32 \n",
            "mapping.fc1           262656      -        [32, 512]            float32 \n",
            "mapping               -           512      [32, 14, 512]        float32 \n",
            "synthesis.b4.conv1    2622465     32       [32, 512, 4, 4]      float32 \n",
            "synthesis.b4.torgb    264195      -        [32, 3, 4, 4]        float32 \n",
            "synthesis.b4:0        8192        16       [32, 512, 4, 4]      float32 \n",
            "synthesis.b4:1        -           -        [32, 512, 4, 4]      float32 \n",
            "synthesis.b8.conv0    2622465     80       [32, 512, 8, 8]      float32 \n",
            "synthesis.b8.conv1    2622465     80       [32, 512, 8, 8]      float32 \n",
            "synthesis.b8.torgb    264195      -        [32, 3, 8, 8]        float32 \n",
            "synthesis.b8:0        -           16       [32, 512, 8, 8]      float32 \n",
            "synthesis.b8:1        -           -        [32, 512, 8, 8]      float32 \n",
            "synthesis.b16.conv0   2622465     272      [32, 512, 16, 16]    float32 \n",
            "synthesis.b16.conv1   2622465     272      [32, 512, 16, 16]    float32 \n",
            "synthesis.b16.torgb   264195      -        [32, 3, 16, 16]      float32 \n",
            "synthesis.b16:0       -           16       [32, 512, 16, 16]    float32 \n",
            "synthesis.b16:1       -           -        [32, 512, 16, 16]    float32 \n",
            "synthesis.b32.conv0   2622465     1040     [32, 512, 32, 32]    float16 \n",
            "synthesis.b32.conv1   2622465     1040     [32, 512, 32, 32]    float16 \n",
            "synthesis.b32.torgb   264195      -        [32, 3, 32, 32]      float16 \n",
            "synthesis.b32:0       -           16       [32, 512, 32, 32]    float16 \n",
            "synthesis.b32:1       -           -        [32, 512, 32, 32]    float32 \n",
            "synthesis.b64.conv0   1442561     4112     [32, 256, 64, 64]    float16 \n",
            "synthesis.b64.conv1   721409      4112     [32, 256, 64, 64]    float16 \n",
            "synthesis.b64.torgb   132099      -        [32, 3, 64, 64]      float16 \n",
            "synthesis.b64:0       -           16       [32, 256, 64, 64]    float16 \n",
            "synthesis.b64:1       -           -        [32, 256, 64, 64]    float32 \n",
            "synthesis.b128.conv0  426369      16400    [32, 128, 128, 128]  float16 \n",
            "synthesis.b128.conv1  213249      16400    [32, 128, 128, 128]  float16 \n",
            "synthesis.b128.torgb  66051       -        [32, 3, 128, 128]    float16 \n",
            "synthesis.b128:0      -           16       [32, 128, 128, 128]  float16 \n",
            "synthesis.b128:1      -           -        [32, 128, 128, 128]  float32 \n",
            "synthesis.b256.conv0  139457      65552    [32, 64, 256, 256]   float16 \n",
            "synthesis.b256.conv1  69761       65552    [32, 64, 256, 256]   float16 \n",
            "synthesis.b256.torgb  33027       -        [32, 3, 256, 256]    float16 \n",
            "synthesis.b256:0      -           16       [32, 64, 256, 256]   float16 \n",
            "synthesis.b256:1      -           -        [32, 64, 256, 256]   float32 \n",
            "---                   ---         ---      ---                  ---     \n",
            "Total                 23191522    175568   -                    -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape         Datatype\n",
            "---            ---         ---      ---                  ---     \n",
            "b256.fromrgb   256         16       [32, 64, 256, 256]   float16 \n",
            "b256.skip      8192        16       [32, 128, 128, 128]  float16 \n",
            "b256.conv0     36928       16       [32, 64, 256, 256]   float16 \n",
            "b256.conv1     73856       16       [32, 128, 128, 128]  float16 \n",
            "b256           -           16       [32, 128, 128, 128]  float16 \n",
            "b128.skip      32768       16       [32, 256, 64, 64]    float16 \n",
            "b128.conv0     147584      16       [32, 128, 128, 128]  float16 \n",
            "b128.conv1     295168      16       [32, 256, 64, 64]    float16 \n",
            "b128           -           16       [32, 256, 64, 64]    float16 \n",
            "b64.skip       131072      16       [32, 512, 32, 32]    float16 \n",
            "b64.conv0      590080      16       [32, 256, 64, 64]    float16 \n",
            "b64.conv1      1180160     16       [32, 512, 32, 32]    float16 \n",
            "b64            -           16       [32, 512, 32, 32]    float16 \n",
            "b32.skip       262144      16       [32, 512, 16, 16]    float16 \n",
            "b32.conv0      2359808     16       [32, 512, 32, 32]    float16 \n",
            "b32.conv1      2359808     16       [32, 512, 16, 16]    float16 \n",
            "b32            -           16       [32, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [32, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [32, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [32, 512, 8, 8]      float32 \n",
            "b16            -           16       [32, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [32, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [32, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [32, 512, 4, 4]      float32 \n",
            "b8             -           16       [32, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [32, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [32, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [32, 512]            float32 \n",
            "b4.out         513         -        [32, 1]              float32 \n",
            "---            ---         ---      ---                  ---     \n",
            "Total          24001089    416      -                    -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "Training for 25000 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 2m 28s       sec/tick 32.7    sec/kimg 1020.79 maintenance 115.2  cpumem 3.94   gpumem 9.78   augment 0.000\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 319.98041287902885}, \"metric\": \"fid50k_full\", \"total_time\": 2189.706902742386, \"total_time_str\": \"36m 30s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000000.pkl\", \"timestamp\": 1648033139.484896}\n",
            "tick 1     kimg 4.0      time 56m 21s      sec/tick 1016.0  sec/kimg 254.01  maintenance 2216.8 cpumem 4.09   gpumem 9.76   augment 0.000\n",
            "tick 2     kimg 8.0      time 1h 13m 20s   sec/tick 1017.8  sec/kimg 254.46  maintenance 1.1    cpumem 4.09   gpumem 9.79   augment 0.001\n"
          ]
        }
      ],
      "source": [
        "%cd /content/stylegan2-ada-pytorch/\n",
        "OUT_DIR=\"/content/drive/MyDrive/AS-out-dir\"\n",
        "TR_DATA=\"/content/train-data\"\n",
        "!python train.py --outdir=$OUT_DIR --data=$TR_DATA --gpus=1 --snap=20 --batch=32 --mirror=1 \n",
        "# --allow-tf32 True --batch=64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt_rHqEpGi9P"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/Shareddrives/Tech/experiments/watercolor/data/curated-1-resized-sg2-ds-256x256.zip ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vlQgMYJ_qcX"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/MyDrive/out_dir/00000-test3-resized-128x128-mirror-auto1-batch32-resumecustom"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of stylegan2-ada.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}